{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28a2f9ce",
   "metadata": {},
   "source": [
    "## üìÅ Understanding the Project Structure\n",
    "\n",
    "This notebook is part of a larger project that contains **many datasets** stored in a shared `data/` folder.\n",
    "\n",
    "Because this notebook lives in the `lessons/` folder, we cannot assume that the current working directory is the project root. Instead, we programmatically locate the project root and then build paths from there.\n",
    "\n",
    "This approach prevents common file-not-found errors and works even if notebooks are moved to different folders.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcf69d4",
   "metadata": {},
   "source": [
    "## üìç Step 1: Locate the Data Directory\n",
    "\n",
    "In this step, we:\n",
    "\n",
    "* Identify the project‚Äôs root directory\n",
    "* Build a reliable path to the `data/` folder\n",
    "* Verify that the folder exists\n",
    "\n",
    "```python\n",
    "REPO_ROOT = Path.cwd().parents[0]\n",
    "DATA_DIR = REPO_ROOT / \"data\"\n",
    "```\n",
    "\n",
    "If the data directory exists, we know the notebook is correctly configured.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0089d59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /workspaces/Data_Science-notebooks/lessons/Jupyter\n",
      "Data directory: /workspaces/Data_Science-notebooks/lessons/data\n",
      "Data directory exists: False\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "DATA_DIR = Path.cwd().parent / \"data\"\n",
    "\n",
    "print(\"Working directory:\", Path.cwd())\n",
    "print(\"Data directory:\", DATA_DIR)\n",
    "print(\"Data directory exists:\", DATA_DIR.exists())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ceeca8d",
   "metadata": {},
   "source": [
    "## üì¶ Step 2: Load All CSV Files Automatically\n",
    "\n",
    "Rather than loading each dataset manually, we scan the `data/` folder and load **every CSV file** into pandas.\n",
    "\n",
    "Each dataset is stored in a dictionary called `dfs`, where:\n",
    "\n",
    "* The **key** is the dataset name\n",
    "* The **value** is a pandas DataFrame\n",
    "\n",
    "This allows us to work with many datasets using a consistent and scalable approach.\n",
    "\n",
    "Missing values such as `\"NA\"` or empty cells are automatically converted to `NaN`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5d9e95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "REPO_ROOT = Path.cwd().parents[0]\n",
    "DATA_DIR = REPO_ROOT / \"data\"\n",
    "\n",
    "csv_files = sorted(DATA_DIR.glob(\"*.csv\"))\n",
    "\n",
    "dfs = {}\n",
    "\n",
    "for f in csv_files:\n",
    "    name = f.stem.replace(\"-\", \"_\")\n",
    "    dfs[name] = pd.read_csv(\n",
    "        f,\n",
    "        na_values=[\"NA\", \"\"],\n",
    "        keep_default_na=True\n",
    "    )\n",
    "    print(f\"Loaded {name:30s} shape={dfs[name].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a55f5d",
   "metadata": {},
   "source": [
    "## ‚úÖ Step 3: Confirm Successful Data Loading\n",
    "\n",
    "As each file is loaded, the notebook prints:\n",
    "\n",
    "* The dataset name\n",
    "* The number of rows\n",
    "* The number of columns\n",
    "\n",
    "Example output:\n",
    "\n",
    "```\n",
    "Loaded cardio10 shape=(67066, 27)\n",
    "```\n",
    "\n",
    "This acts as a **sanity check** to confirm that all files loaded correctly.\n",
    "\n",
    "If a dataset is missing or malformed, it will be immediately obvious.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc5022a",
   "metadata": {},
   "source": [
    "## üóÇÔ∏è Step 4: View a Catalog of Available Datasets\n",
    "\n",
    "After loading the data, we create a summary table that lists:\n",
    "\n",
    "* Dataset name\n",
    "* Number of rows\n",
    "* Number of columns\n",
    "\n",
    "This table functions as a lightweight **data catalog** and helps you quickly understand what data is available for analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3ff8951",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[32m/tmp/ipykernel_14208/3499651979.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      4\u001b[39m     pd.DataFrame(\n\u001b[32m      5\u001b[39m         [{\"dataset\": name, \"rows\": df.shape[0], \"cols\": df.shape[1]}\n\u001b[32m      6\u001b[39m          \u001b[38;5;28;01mfor\u001b[39;00m name, df \u001b[38;5;28;01min\u001b[39;00m dfs.items()]\n\u001b[32m      7\u001b[39m     )\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     .sort_values(\u001b[33m\"dataset\"\u001b[39m)  \u001b[38;5;66;03m# alphabetical\u001b[39;00m\n\u001b[32m      9\u001b[39m     .reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     10\u001b[39m )\n\u001b[32m     11\u001b[39m \n",
      "\u001b[32m/usr/local/lib/python3.12/site-packages/pandas/core/frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[39m\n\u001b[32m   8343\u001b[39m             )\n\u001b[32m   8344\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m by:\n\u001b[32m   8345\u001b[39m             \u001b[38;5;66;03m# len(by) == 1\u001b[39;00m\n\u001b[32m   8346\u001b[39m \n\u001b[32m-> \u001b[39m\u001b[32m8347\u001b[39m             k = self._get_label_or_level_values(by[\u001b[32m0\u001b[39m], axis=axis)\n\u001b[32m   8348\u001b[39m \n\u001b[32m   8349\u001b[39m             \u001b[38;5;66;03m# need to rewrap column in Series to apply key function\u001b[39;00m\n\u001b[32m   8350\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m/usr/local/lib/python3.12/site-packages/pandas/core/generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1772\u001b[39m             values = self.xs(key, axis=first_other_axes)._values\n\u001b[32m   1773\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m self._is_level_reference(key, axis=axis):\n\u001b[32m   1774\u001b[39m             values = self.axes[axis].get_level_values(key)._values\n\u001b[32m   1775\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m KeyError(key)\n\u001b[32m   1777\u001b[39m \n\u001b[32m   1778\u001b[39m         \u001b[38;5;66;03m# Check for duplicates\u001b[39;00m\n\u001b[32m   1779\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m values.ndim > \u001b[32m1\u001b[39m:\n",
      "\u001b[31mKeyError\u001b[39m: 'dataset'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "catalog = (\n",
    "    pd.DataFrame(\n",
    "        [{\"dataset\": name, \"rows\": df.shape[0], \"cols\": df.shape[1]}\n",
    "         for name, df in dfs.items()]\n",
    "    )\n",
    "    .sort_values(\"dataset\")  # alphabetical\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "catalog\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2269450",
   "metadata": {},
   "source": [
    "## üëÄ Step 5: Preview Individual Datasets\n",
    "\n",
    "To explore a dataset, we can display:\n",
    "\n",
    "* The first few rows\n",
    "* Column names\n",
    "* Dataset dimensions\n",
    "* Missing values\n",
    "\n",
    "This is an important step in **exploratory data analysis (EDA)** and helps you understand:\n",
    "\n",
    "* What each variable represents\n",
    "* Which variables may require cleaning or transformation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2da4bb99",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/workspaces/Data_Science-notebooks/lessons/data/cardio10.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m DATA_DIR = Path.cwd().parent / \u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Load one dataset directly\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_DIR\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcardio10.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mNA\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Preview the first 5 rows\u001b[39;00m\n\u001b[32m     11\u001b[39m df.head()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/pandas/io/parsers/readers.py:873\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, skip_blank_lines, parse_dates, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m    861\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m    862\u001b[39m     dialect,\n\u001b[32m    863\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m    869\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m    870\u001b[39m )\n\u001b[32m    871\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/pandas/io/parsers/readers.py:300\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    297\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    299\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    303\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1645\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1642\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1644\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1645\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1904\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1902\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1903\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1904\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1905\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1906\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1907\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1908\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1909\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1910\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1911\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1912\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1913\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1914\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1915\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/pandas/io/common.py:926\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    921\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    922\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    923\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    924\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m926\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    934\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    935\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/workspaces/Data_Science-notebooks/lessons/data/cardio10.csv'"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Locate the data directory\n",
    "DATA_DIR = Path.cwd().parent / \"data\"\n",
    "\n",
    "# Load one dataset directly\n",
    "df = pd.read_csv(DATA_DIR / \"cardio10.csv\", na_values=[\"NA\", \"\"])\n",
    "\n",
    "# Preview the first 5 rows\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "155b68bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67066, 27)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72e40d65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'age_days', 'sex', 'height', 'weight', 'sbp', 'diastolic',\n",
       "       'cholesterol', 'gluc', 'smoking', 'alco', 'active', 'cardio', 'age',\n",
       "       'bmi', 'tc', 'hdl', 'dm', 'egfr', 'bptreat', 'statin', 'uacr', 'sdi',\n",
       "       'hba1c', 'prevent_full_10yr_CVD', 'prevent_full_10yr_ASCVD',\n",
       "       'prevent_full_10yr_HF'],\n",
       "      dtype='str')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c926e76a",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Step 6: Understanding Missing Values\n",
    "\n",
    "Some datasets intentionally contain missing values.\n",
    "\n",
    "These are not errors ‚Äî they represent real-world data challenges that analysts must address.\n",
    "\n",
    "In this course, you will:\n",
    "\n",
    "* Identify missing values\n",
    "* Decide how to handle them (drop, impute, or analyze separately)\n",
    "* Learn how missing data affects analysis and modeling\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fcca639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                         0\n",
       "age_days                   0\n",
       "sex                        0\n",
       "height                     0\n",
       "weight                     0\n",
       "sbp                        0\n",
       "diastolic                  0\n",
       "cholesterol                0\n",
       "gluc                       0\n",
       "smoking                    0\n",
       "alco                       0\n",
       "active                     0\n",
       "cardio                     0\n",
       "age                        0\n",
       "bmi                        0\n",
       "tc                         0\n",
       "hdl                        0\n",
       "dm                         0\n",
       "egfr                       0\n",
       "bptreat                    0\n",
       "statin                     0\n",
       "uacr                       0\n",
       "sdi                        0\n",
       "hba1c                      0\n",
       "prevent_full_10yr_CVD      0\n",
       "prevent_full_10yr_ASCVD    0\n",
       "prevent_full_10yr_HF       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f4df61",
   "metadata": {},
   "source": [
    "## üß† Why This Workflow Matters\n",
    "\n",
    "This notebook demonstrates best practices used by professional data scientists:\n",
    "\n",
    "* Writing **reusable code**\n",
    "* Avoiding hard-coded file paths\n",
    "* Verifying data before analysis\n",
    "* Separating data loading from modeling\n",
    "\n",
    "Mastering this workflow will make your future analyses:\n",
    "\n",
    "* More reliable\n",
    "* Easier to debug\n",
    "* Easier to scale to larger projects\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ What Comes Next\n",
    "\n",
    "With all datasets loaded, you are now ready to:\n",
    "\n",
    "* Explore relationships between variables\n",
    "* Join datasets together\n",
    "* Clean and transform data\n",
    "* Build statistical or machine learning models\n",
    "\n",
    "Each future lesson will build on the foundation established here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
